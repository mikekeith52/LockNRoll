
# Double Deep Q Learning

Using a framework obsensibly developed by [Greg Surma](https://gsurma.medium.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f), I trained an AI to learn Lock N' Roll. There were several barriers to overcome, including how to restrict the action space, since not all moves are available at all times in a real game, how to feed a reasonably sized observation space, and how to have it respond to the random nature of the game. My solution was by no means perfect, and I am still in the process of developing the learning agent through much trial and tribululation, but after hundreds of thousands of training iterations, there is evidence that it is better than a random player. It is nowhere near a good human's level, who can consistently score over 10,000 (even under the restricted action space implemented for the AI), but it is good for about 200 points per game on average and regularly scores over 2,000. A random agent scores about 165 and rarely scores over 1,000.  

From what I understand about Deep Q Learning, it uses a process of chaining moves together and scoring any given move not only by the reward it immediately collects, but also by a discounted predicted future state that the current move will create. In effect, each prediction is based on a markov chain under a Bayesian framework. Double Deep Q Learning goes one step further by systematically unbiasing certain moves that the agent will begin to favor over time just because it's a move it's selected many times before. The unbiasing process is accomplished with a second neural network and it helps balance the universe of available moves to the agent so it is constantly learning what moves are actually best. In my mind, this is a perfect framework to learn Lock N' Roll. My challenge is figuring out how to let the computer know that certain moves are not always available. The examples I found online were not always clear about how to solve this problem, although the answer may lie with [Muzero](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py). My current best solution is to always allow the agent to play one of 16 moves. It will place the next available die, or joker if no more dice are available, on the closest space to the number between 0-15 it selects. It lock and rolls after every placement. These restrictions do make the game much more challening, but even with them implemented, it is not unreasonable to expect a total score of over 10,000 points by a good player.  

Unfortunately, after 500k+ training iterations, the AI never reached the threshold I was hoping for, but it is showing signs of constantly improving. If I had a better processor, I'd keep training it up to 5 million times and assess its progress. As it stands, it seems to know some patterns will score it points, but not others. The problem I run into is it takes way too long to train on my dinky Windows Surface laptop and it heats up the processor to an unsustainable level. I'm already worried that I've significantly shortened the life on my hardware just from the training I did force it to undertake.  

I also built the observation space based on all information a real player would have. This ended up being 193 one-hot encoded variables, with a large degree of redudancy (because if a blue is on space 1, a red, green, or yellow is not, but you still need those three variables represented--there is no way to force an ordinal relationship between colors or even numbers in this game universe). The lock n' roll problem ends up being extremely multi-dimensional that we process in our minds very trivially, but which is very hard to program into a computer. I reduced those 193 variables to 154 with PCA (keeping 99.9%+ of the dataset's variance), but I think I need to learn more about LSTM auto-encoders to obtain even better results. I'm also thinking about using a genetic process to find the best possible neural network estimator. So much to consider!  

This is something I will keep developing. For now, it is available to help you play in app.py with selection 5. You can also analyze some of its meta stats by running the analyze.ipynb notebook.  