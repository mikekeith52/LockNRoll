
# Double Deep Q Learning

Using a framework obsensibly developed by [Greg Surma](https://gsurma.medium.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f), I trained an AI to learn Lock N' Roll. There were several barriers to overcome, including how to restrict the action space, since not all moves are available at all times in a real game; how to feed a reasonably sized observation space; and how to have it respond to the random nature of the game. My solutions are by no means perfect, and I am still in the process of developing the learning agent through much trial and tribululation, but after hundreds of thousands of training iterations, there is evidence that it did learn something (big win). It is nowhere near a good human's level, who can consistently score over 10,000 (even under the restricted rules implemented for the AI), but it is good for about 200 points per game on average and regularly scores over 2,000. A random agent scores about 165 and rarely scores over 1,000.  

From what I understand about Deep Q Learning, it uses a process of chaining moves together and scoring any given move not only by the reward it immediately collects, but also by a discounted predicted future state that the current move will create. In effect, each prediction is based on a markov chain under a Bayesian framework. I decided to reward each move the actual points collected from that move where game overs are worth -500.

Double Deep Q Learning goes one step further by systematically unbiasing certain moves that the agent will begin to favor over time just because it's a move it's selected many times before. The unbiasing process is accomplished with a second neural network and it helps balance the universe of available moves to the agent so it is constantly learning what moves are actually best. In my mind, this is a perfect framework to learn Lock N' Roll. My challenge was figuring out how to let the computer know that certain moves are not always available. The examples I found online were not always clear about how to solve this problem and many discussions on online forums seemed to suggest this framework is much simpler and more effective when you don't have to restrict the action space, although a better answer may lie with the [Muzero pseudocode](https://arxiv.org/src/1911.08265v1/anc/pseudocode.py) which I'm still working through.

My current solution is to not restrict the action space by always allowing the agent to play one of 16 moves. It will place the next available die (and the die are always sorted alphabetically), or joker if no dice are available, on the closest space to the number between 0-15 it selects. It lock and rolls after every placement. These restrictions do make the game significantly more challening, but even with them implemented, it is not unreasonable to expect a total score of over 10,000 points by a good player.  

Unfortunately, after 500k+ training iterations, the AI never reached the threshold I was hoping for, but it is showing signs of constantly improving. If I had a better processor, I'd keep training it up to 5 million times and assess its progress. As it stands, it seems to know some patterns will score points, but not others. The problem I run into is it takes way too long to train on my dinky Windows Surface laptop and it heats up the processor to an unsustainable level. I'm already worried that I've significantly shortened the life on my hardware just from the training I've already asked it to undertake.  

I also built the observation space based on all information a real player would have. This ended up being 193 one-hot encoded variables, with a large degree of redudancy in them (because if a blue is on board space 1, a red, green, or yellow is not, but you still need those three variables represented--there is no way to force an ordinal relationship between colors or even numbers in this game's universe). On top of that, there are many different patterns and placements the computer needs to consider in order to maximize its reward without an obvious ordinal relationship between them. The lock n' roll problem ends up being extremely multi-dimensional that we process in our minds trivially, but which is quite difficult to program into a computer. I reduced those 193 variables to 154 with PCA (keeping 99.9%+ of the dataset's variance), but I think I need to learn more about LSTM auto-encoders to obtain better results. I'm also thinking about using a genetic process to find the best possible neural network estimator. So much to consider!  

This is something I will keep developing. For now, it is available to help you play in app.py with selection 5. You can also analyze some of its meta stats by running the analyze.ipynb notebook.  